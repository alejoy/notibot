import requests
from bs4 import BeautifulSoup
from urllib.parse import urljoin
import time
import re
import os
import logging

# Configurar logging
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)

# Telegram Config - usar variables de entorno
TELEGRAM_BOT_TOKEN = os.getenv('TELEGRAM_BOT_TOKEN')
OPENROUTER_API_KEY = os.getenv('OPENROUTER_API_KEY')

# Validar que existan las variables
if not TELEGRAM_BOT_TOKEN:
    logger.error("‚ùå Falta TELEGRAM_BOT_TOKEN en variables de entorno")
    exit(1)
    
if not OPENROUTER_API_KEY:
    logger.error("‚ùå Falta OPENROUTER_API_KEY en variables de entorno")
    exit(1)

logger.info("‚úÖ Variables de entorno cargadas correctamente")

HEADERS = {
    "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/115.0.0.0 Safari/537.36"
}

# Configuraci√≥n de sitios
SITIOS = [
    {
        "nombre": "R√≠o Negro",
        "url": "https://www.rionegro.com.ar/politica/",
        "content_selector": {
            "tag": "div",
            "class_": "newsfull__body"
        },
        "link_pattern": "/politica/"
    },
    {
        "nombre": "P√°gina/12",
        "url": "https://www.pagina12.com.ar/secciones/el-pais",
        "content_selector": {
            "tag": "div",
            "class_": "article-main-content"
        },
        "link_pattern_regex": r"^\/\d{6,}-.+"
    },
    {
        "nombre": "La Naci√≥n",
        "url": "https://www.lanacion.com.ar/politica/",
        "content_selector": {
            "tag": "div",
            "class_": "com-paragraph"
        },
        "link_pattern": "/politica/"
    },
    {
        "nombre": "Infobae",
        "url": "https://www.infobae.com/politica/",
        "content_selector": {
            "tag": "figcaption",
            "class_": "article-figcaption-img"
        },
        "link_pattern": "/politica/"
    }
]

# Configuraci√≥n de tonos
TONOS = {
    "libertario": "Analiz√° esta noticia desde una perspectiva libertaria, enfoc√°ndote en la libertad individual, el libre mercado y la limitaci√≥n del Estado.",
    "cr√≠tico al neoliberalismo": "Analiz√° esta noticia con una perspectiva cr√≠tica al neoliberalismo, enfoc√°ndote en desigualdades sociales y el rol del Estado en la protecci√≥n social.",
    "neutral informativo": "Resum√≠ esta noticia de forma objetiva y neutral, presentando los hechos principales sin sesgo pol√≠tico."
}

TONOS_POSIBLES = ["libertario", "cr√≠tico al neoliberalismo", "neutral informativo"]

# --- FUNCIONES ---

SUBSCRIBERS_FILE = "subscribers.txt"

def obtener_chat_ids():
    SUPABASE_URL = os.getenv("SUPABASE_URL")
    SUPABASE_KEY = os.getenv("SUPABASE_ANON_KEY")

    try:
        response = requests.get(
            f"{SUPABASE_URL}/rest/v1/subscribers",
            headers={
                "apikey": SUPABASE_KEY,
                "Authorization": f"Bearer {SUPABASE_KEY}"
            }
        )
        if response.status_code == 200:
            datos = response.json()
            return [item["chat_id"] for item in datos]
        else:
            print(f"‚ùå Error al obtener chat_ids: {response.text}")
            return []
    except Exception as e:
        print(f"‚ùå Error: {e}")
        return []
    
def enviar_telegram(mensaje, chat_id):
    """Env√≠a un mensaje por Telegram"""
    url = f"https://api.telegram.org/bot{TELEGRAM_BOT_TOKEN}/sendMessage"
    data = {
        "chat_id": chat_id,
        "text": mensaje,
        "parse_mode": "Markdown"
    }
    try:
        response = requests.post(url, data=data)
        if response.status_code == 200:
            print(f"‚úÖ Mensaje enviado a {chat_id}")
            return True
        else:
            print(f"‚ùå Error enviando mensaje: {response.text}")
            return False
    except Exception as e:
        print(f"‚ùå Excepci√≥n al enviar mensaje por Telegram: {e}")
        return False

def obtener_enlaces(sitio):
    """
    Obtiene los enlaces de noticias de la p√°gina principal de un sitio.
    Maneja tanto patrones simples como regex para P√°gina/12.
    """
    url = sitio["url"]
    nombre = sitio["nombre"]
    print(f"üì• Obteniendo enlaces de {nombre}...")
    
    try:
        response = requests.get(url, headers=HEADERS, timeout=10)
        response.raise_for_status()
        soup = BeautifulSoup(response.content, "html.parser")
        enlaces = set()

        for link in soup.find_all("a", href=True):
            href = link["href"]
            is_article = False
            
            # Manejo especial para P√°gina/12 con regex
            if "link_pattern_regex" in sitio:
                pattern_regex = sitio["link_pattern_regex"]
                if re.match(pattern_regex, href):
                    is_article = True
            
            # Manejo para otros sitios con pattern simple
            elif "link_pattern" in sitio:
                pattern = sitio["link_pattern"]
                if pattern in href:
                    is_article = True

            if is_article:
                full_url = urljoin(url, href)
                enlaces.add(full_url)

        print(f"‚úÖ Encontrados {len(enlaces)} enlaces √∫nicos para {nombre}.")
        return list(enlaces)[:3]  # Limitar a 3 noticias por sitio

    except requests.exceptions.RequestException as e:
        print(f"‚ùå Error al conectar con {url}: {e}")
        return []

def extraer_contenido(url, selector):
    """
    Extrae el texto de un art√≠culo.
    Maneja diferentes tipos de selectores para cada sitio.
    """
    print(f"   üìÑ Extrayendo contenido de: {url[:70]}...")
    try:
        response = requests.get(url, headers=HEADERS, timeout=10)
        response.raise_for_status()
        soup = BeautifulSoup(response.text, 'html.parser')

        # Construcci√≥n din√°mica de los argumentos para find()
        find_args = {}
        if 'class_' in selector:
            find_args['class_'] = selector['class_']
        if 'attrs' in selector:
            find_args['attrs'] = selector['attrs']

        # Buscar el contenedor principal
        contenedor = soup.find(selector['tag'], **find_args)

        if not contenedor:
            print(f"   ‚ö†Ô∏è No se encontr√≥ el contenedor principal. Intentando alternativas...")
            
            # Fallback: buscar contenedores alternativos comunes
            fallback_selectors = [
                {'tag': 'div', 'class_': 'content'},
                {'tag': 'div', 'class_': 'article-body'},
                {'tag': 'div', 'class_': 'post-content'},
                {'tag': 'article'},
                {'tag': 'main'}
            ]
            
            for fallback in fallback_selectors:
                fb_args = {}
                if 'class_' in fallback:
                    fb_args['class_'] = fallback['class_']
                
                contenedor = soup.find(fallback['tag'], **fb_args)
                if contenedor:
                    print(f"   ‚úÖ Usando selector alternativo: {fallback}")
                    break
            
            if not contenedor:
                print(f"   ‚ùå No se pudo encontrar contenido con ning√∫n selector")
                return None

        # Limpiar el contenido
        # Remover scripts, styles y otros elementos no deseados
        for script in contenedor(["script", "style", "nav", "footer", "header"]):
            script.decompose()

        # Extraer texto limpio
        texto = ' '.join(contenedor.get_text(separator=' ', strip=True).split())
        
        # Validar que el texto tenga contenido m√≠nimo
        if len(texto) < 100:
            print(f"   ‚ö†Ô∏è Contenido muy corto ({len(texto)} caracteres)")
            return None
            
        return texto

    except requests.exceptions.RequestException as e:
        print(f"‚ùå Error al extraer contenido de {url}: {e}")
        return None

def resumir_con_tono(texto, tono):
    """
    Genera un resumen del texto con el tono especificado usando OpenRouter
    """
    headers = {
        "Authorization": f"Bearer {OPENROUTER_API_KEY}",
        "Content-Type": "application/json"
    }
    
    prompt = TONOS.get(tono, "Resum√≠ el siguiente texto de forma clara y breve.") + f"\n\n{texto[:4000]}"
    
    data = {
        "model": "mistralai/mixtral-8x7b-instruct",
        "messages": [
            {"role": "system", "content": f"Sos un analista pol√≠tico con enfoque {tono}."},
            {"role": "user", "content": prompt}
        ],
        "max_tokens": 300
    }
    
    for intento in range(3):
        try:
            response = requests.post(
                "https://openrouter.ai/api/v1/chat/completions", 
                headers=headers, 
                json=data, 
                timeout=40
            )
            if response.status_code == 200:
                return response.json()['choices'][0]['message']['content'].strip()
            else:
                print(f"‚ö†Ô∏è Intento {intento+1} fallido: {response.status_code} - {response.text}")
        except Exception as e:
            print(f"‚ö†Ô∏è Error en intento {intento+1}: {e}")
        
        # Pausa entre intentos
        time.sleep(2)
    
    return "[No se pudo generar resumen]"

def ejecutar_bot():
    """
    Funci√≥n principal que ejecuta el bot
    """
    print("ü§ñ Iniciando bot de noticias...")

    suscriptores = cargar_chat_ids()
    if not suscriptores:
        print("‚ö†Ô∏è No hay suscriptores registrados en subscribers.txt")
        return
    
    for sitio in SITIOS:
        print(f"\nüåê Procesando sitio: {sitio['nombre']}")
        enlaces = obtener_enlaces(sitio)
        
        if not enlaces:
            print(f"No se pudieron obtener enlaces para {sitio['nombre']}. Saltando al siguiente.\n")
            continue
        
        for link in enlaces:
            print(f"üîó Procesando: {link}")
            contenido = extraer_contenido(link, sitio['content_selector'])
            
            if contenido:
                resumenes = []
                
                for tono in TONOS_POSIBLES:
                    print(f"   üé≠ Generando resumen con tono: {tono}")
                    resumen = resumir_con_tono(contenido, tono)
                    
                    if resumen and resumen != "[No se pudo generar resumen]":
                        resumenes.append(f"üó£ *{tono.capitalize()}*\n{resumen}")
                    else:
                        resumenes.append(f"üó£ *{tono.capitalize()}*\n[No se pudo generar resumen]")
                
                # Construir mensaje final
                mensaje = f"üì∞ *{sitio['nombre']} - Comparativa de enfoques*\n\n" + \
                          "\n\n".join(resumenes) + \
                          f"\n\nüîó {link}"

                # Enviar a cada suscriptor
                 chat_ids = obtener_chat_ids()
                 for chat_id in chat_ids:
                    if enviar_telegram(mensaje, chat_id):
                        print(f"‚úÖ Enviado a {chat_id}")
                    else:
                        print(f"‚ùå Fall√≥ env√≠o a {chat_id}")
                
                time.sleep(5)
            else:
                print("‚ö†Ô∏è No se pudo extraer contenido del art√≠culo.")
    
    print("\nüèÅ Bot finalizado.")

def test_scraping():
    """
    Funci√≥n de prueba para verificar el scraping sin usar APIs
    """
    print("üß™ Modo de prueba - Solo scraping")
    todas_las_noticias = []
    
    for sitio in SITIOS:
        enlaces_articulos = obtener_enlaces(sitio)
        if not enlaces_articulos:
            print(f"No se pudieron obtener enlaces para {sitio['nombre']}. Saltando al siguiente.\n")
            continue

        for enlace in enlaces_articulos:
            contenido = extraer_contenido(enlace, sitio["content_selector"])
            if contenido:
                noticia = {
                    "fuente": sitio["nombre"],
                    "url": enlace,
                    "contenido": contenido[:500] + "..."
                }
                todas_las_noticias.append(noticia)
                print(f"   üëç Contenido extra√≠do de {sitio['nombre']}.\n")
            else:
                print(f"   üëé No se pudo extraer contenido para el enlace: {enlace}\n")
    
    print("\n--- RESULTADO FINAL DEL SCRAPING ---")
    for i, noticia in enumerate(todas_las_noticias, 1):
        print(f"\nNoticia #{i}:")
        print(f"  Fuente: {noticia['fuente']}")
        print(f"  URL: {noticia['url']}")
        print(f"  Contenido: {noticia['contenido']}")

# --- EJECUCI√ìN ---
if __name__ == "__main__":
    # Descomenta la funci√≥n que quieras ejecutar:
    
    # Para pruebas sin APIs:
    # test_scraping()
    
    # Para ejecuci√≥n completa del bot:
    ejecutar_bot()
